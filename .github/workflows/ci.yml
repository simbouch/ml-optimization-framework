name: ML Optimization Framework CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]

env:
  PYTHON_VERSION: "3.11"

jobs:
  # Code Quality and Validation
  code-quality:
    runs-on: ubuntu-latest
    name: Code Quality & Validation

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-minimal.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-minimal.txt
        pip install black flake8 mypy isort bandit safety pytest

    - name: Create necessary directories
      run: |
        mkdir -p logs results studies

    - name: Run code formatting check (Black)
      run: black --check --diff *.py
      continue-on-error: true

    - name: Run import sorting check (isort)
      run: isort --check-only --diff *.py
      continue-on-error: true

    - name: Run linting (Flake8)
      run: flake8 *.py --max-line-length=88 --extend-ignore=E203,W503
      continue-on-error: true

    - name: Run type checking (MyPy)
      run: mypy *.py --ignore-missing-imports --no-strict-optional
      continue-on-error: true

    - name: Run security check (Bandit)
      run: bandit -r *.py -f json -o bandit-report.json
      continue-on-error: true

    - name: Run dependency security check (Safety)
      run: safety check --json --output safety-report.json
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Application Testing
  application-tests:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ["3.10", "3.11", "3.12"]
        exclude:
          # Reduce matrix size for faster CI
          - os: windows-latest
            python-version: "3.10"

    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements-minimal.txt') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-minimal.txt
        pip install pytest pytest-timeout

    - name: Create necessary directories
      run: |
        mkdir -p logs results studies

    - name: Test Python syntax
      run: |
        python -m py_compile create_unified_demo.py

    - name: Test package imports
      run: |
        python -c "import optuna, pandas, numpy, sklearn, plotly; print('✅ All packages imported successfully')"

    - name: Test demo study creation
      run: |
        python create_unified_demo.py

    - name: Run basic functionality tests
      run: |
        python -c "
        import optuna
        study = optuna.create_study(direction='minimize')
        def objective(trial):
            x = trial.suggest_float('x', -10, 10)
            return x ** 2
        study.optimize(objective, n_trials=5)
        print(f'✅ Optuna test completed with {len(study.trials)} trials')
        print(f'✅ Best value: {study.best_value:.4f}')
        "

  # Framework Validation
  framework-validation:
    runs-on: ubuntu-latest
    name: Framework Validation
    needs: [code-quality, application-tests]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-minimal.txt

    - name: Create necessary directories
      run: |
        mkdir -p logs results studies

    - name: Run framework validation
      run: |
        python create_unified_demo.py

  # Docker Build and Test
  docker-build-test:
    runs-on: ubuntu-latest
    name: Docker Build & Test
    needs: [code-quality, application-tests]

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image
      run: |
        docker build -t ml-optimization-framework:test .

    - name: Test Docker image basic functionality
      run: |
        docker run --rm ml-optimization-framework:test python -c "
        import optuna, pandas, numpy, sklearn, plotly
        print('✅ All packages available in Docker image')
        "

    - name: Test Docker Compose services
      run: |
        # Create necessary directories
        mkdir -p studies results logs

        # Start services in background
        docker-compose up -d

        # Wait for services to be ready
        sleep 45

        # Test Optuna Dashboard service
        curl -f http://localhost:8080 || echo "Optuna Dashboard service check failed"

        # Cleanup
        docker-compose down
