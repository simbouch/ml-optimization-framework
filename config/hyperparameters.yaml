# Hyperparameter Search Spaces Configuration
# This file defines the search spaces for different machine learning models

hyperparameter_spaces:
  
  # Random Forest Classifier
  random_forest:
    n_estimators:
      type: "int"
      low: 50
      high: 500
      step: 50
      description: "Number of trees in the forest"
    
    max_depth:
      type: "int"
      low: 3
      high: 20
      description: "Maximum depth of the tree"
    
    min_samples_split:
      type: "int"
      low: 2
      high: 20
      description: "Minimum number of samples required to split an internal node"
    
    min_samples_leaf:
      type: "int"
      low: 1
      high: 10
      description: "Minimum number of samples required to be at a leaf node"
    
    max_features:
      type: "categorical"
      choices: ["sqrt", "log2", null]
      description: "Number of features to consider when looking for the best split"
    
    bootstrap:
      type: "categorical"
      choices: [true, false]
      description: "Whether bootstrap samples are used when building trees"
    
    class_weight:
      type: "categorical"
      choices: [null, "balanced"]
      description: "Weights associated with classes"
    
    max_leaf_nodes:
      type: "int"
      low: 10
      high: 1000
      log: true
      description: "Maximum number of leaf nodes"
    
    min_impurity_decrease:
      type: "float"
      low: 0.0
      high: 0.1
      description: "Minimum impurity decrease required for split"

  # XGBoost Classifier
  xgboost:
    n_estimators:
      type: "int"
      low: 50
      high: 500
      step: 50
      description: "Number of boosting rounds"
    
    max_depth:
      type: "int"
      low: 3
      high: 10
      description: "Maximum depth of a tree"
    
    learning_rate:
      type: "float"
      low: 0.01
      high: 0.3
      log: true
      description: "Boosting learning rate"
    
    subsample:
      type: "float"
      low: 0.6
      high: 1.0
      description: "Subsample ratio of the training instances"
    
    colsample_bytree:
      type: "float"
      low: 0.6
      high: 1.0
      description: "Subsample ratio of columns when constructing each tree"
    
    reg_alpha:
      type: "float"
      low: 1e-8
      high: 10.0
      log: true
      description: "L1 regularization term on weights"
    
    reg_lambda:
      type: "float"
      low: 1e-8
      high: 10.0
      log: true
      description: "L2 regularization term on weights"
    
    min_child_weight:
      type: "int"
      low: 1
      high: 10
      description: "Minimum sum of instance weight needed in a child"
    
    gamma:
      type: "float"
      low: 0
      high: 5
      description: "Minimum loss reduction required to make a further partition"
    
    booster:
      type: "categorical"
      choices: ["gbtree", "gblinear"]
      description: "Which booster to use"
    
    scale_pos_weight:
      type: "float"
      low: 0.1
      high: 10.0
      log: true
      description: "Balancing of positive and negative weights"

  # LightGBM Classifier
  lightgbm:
    n_estimators:
      type: "int"
      low: 50
      high: 500
      step: 50
      description: "Number of boosted trees to fit"
    
    max_depth:
      type: "int"
      low: 3
      high: 15
      description: "Maximum tree depth for base learners"
    
    learning_rate:
      type: "float"
      low: 0.01
      high: 0.3
      log: true
      description: "Boosting learning rate"
    
    num_leaves:
      type: "int"
      low: 10
      high: 300
      description: "Maximum tree leaves for base learners"
    
    subsample:
      type: "float"
      low: 0.6
      high: 1.0
      description: "Subsample ratio of the training instance"
    
    colsample_bytree:
      type: "float"
      low: 0.6
      high: 1.0
      description: "Subsample ratio of columns when constructing each tree"
    
    reg_alpha:
      type: "float"
      low: 1e-8
      high: 10.0
      log: true
      description: "L1 regularization term on weights"
    
    reg_lambda:
      type: "float"
      low: 1e-8
      high: 10.0
      log: true
      description: "L2 regularization term on weights"
    
    min_child_samples:
      type: "int"
      low: 5
      high: 100
      description: "Minimum number of data needed in a child"
    
    min_child_weight:
      type: "float"
      low: 1e-5
      high: 10.0
      log: true
      description: "Minimum sum of instance weight needed in a child"
    
    boosting_type:
      type: "categorical"
      choices: ["gbdt", "dart", "goss"]
      description: "Type of boosting algorithm"
    
    class_weight:
      type: "categorical"
      choices: [null, "balanced"]
      description: "Weights associated with classes"

  # Logistic Regression (Optional)
  logistic_regression:
    C:
      type: "float"
      low: 1e-4
      high: 100.0
      log: true
      description: "Inverse of regularization strength"
    
    penalty:
      type: "categorical"
      choices: ["l1", "l2", "elasticnet"]
      description: "Norm used in the penalization"
    
    solver:
      type: "categorical"
      choices: ["liblinear", "saga", "lbfgs"]
      description: "Algorithm to use in the optimization problem"
    
    max_iter:
      type: "int"
      low: 100
      high: 1000
      step: 100
      description: "Maximum number of iterations"
    
    class_weight:
      type: "categorical"
      choices: [null, "balanced"]
      description: "Weights associated with classes"

  # Support Vector Machine (Optional)
  svm:
    C:
      type: "float"
      low: 1e-3
      high: 100.0
      log: true
      description: "Regularization parameter"
    
    kernel:
      type: "categorical"
      choices: ["linear", "poly", "rbf", "sigmoid"]
      description: "Kernel type to be used in the algorithm"
    
    gamma:
      type: "categorical"
      choices: ["scale", "auto"]
      description: "Kernel coefficient for rbf, poly and sigmoid"
    
    degree:
      type: "int"
      low: 2
      high: 5
      description: "Degree of the polynomial kernel function"
    
    class_weight:
      type: "categorical"
      choices: [null, "balanced"]
      description: "Weights associated with classes"

# Conditional Parameters
# These parameters are only used when certain conditions are met
conditional_parameters:
  
  xgboost:
    # Tree-specific parameters (only for gbtree booster)
    tree_specific:
      condition: "booster == 'gbtree'"
      parameters: ["max_depth", "gamma", "min_child_weight", "subsample", "colsample_bytree"]
    
    # Linear-specific parameters (only for gblinear booster)
    linear_specific:
      condition: "booster == 'gblinear'"
      parameters: ["reg_alpha", "reg_lambda"]
  
  lightgbm:
    # DART-specific parameters
    dart_specific:
      condition: "boosting_type == 'dart'"
      parameters:
        drop_rate:
          type: "float"
          low: 0.1
          high: 0.5
          description: "Dropout rate for DART"
        skip_drop:
          type: "float"
          low: 0.1
          high: 0.9
          description: "Probability of skipping dropout"
    
    # GOSS-specific parameters
    goss_specific:
      condition: "boosting_type == 'goss'"
      parameters:
        top_rate:
          type: "float"
          low: 0.1
          high: 0.5
          description: "Retain ratio of large gradient data"
        other_rate:
          type: "float"
          low: 0.05
          high: 0.2
          description: "Retain ratio of small gradient data"
  
  random_forest:
    # Bootstrap-specific parameters
    bootstrap_specific:
      condition: "bootstrap == true"
      parameters:
        oob_score:
          type: "categorical"
          choices: [true, false]
          description: "Whether to use out-of-bag samples to estimate accuracy"
    
    # Non-bootstrap parameters
    no_bootstrap_specific:
      condition: "bootstrap == false"
      parameters:
        max_samples:
          type: "float"
          low: 0.5
          high: 1.0
          description: "Number of samples to draw from X to train each tree"

# Parameter Constraints
# Define relationships and constraints between parameters
parameter_constraints:
  
  lightgbm:
    # num_leaves should be less than 2^max_depth
    num_leaves_constraint:
      description: "num_leaves should be less than 2^max_depth"
      constraint: "num_leaves < 2**max_depth"
  
  random_forest:
    # min_samples_leaf should be <= min_samples_split
    samples_constraint:
      description: "min_samples_leaf should be <= min_samples_split"
      constraint: "min_samples_leaf <= min_samples_split"

# Search Space Presets
# Predefined search spaces for different scenarios
search_space_presets:
  
  quick_search:
    description: "Reduced search space for quick optimization"
    models:
      random_forest:
        n_estimators: {type: "int", low: 50, high: 200, step: 50}
        max_depth: {type: "int", low: 3, high: 10}
        min_samples_split: {type: "int", low: 2, high: 10}
      
      xgboost:
        n_estimators: {type: "int", low: 50, high: 200, step: 50}
        max_depth: {type: "int", low: 3, high: 8}
        learning_rate: {type: "float", low: 0.05, high: 0.2, log: true}
  
  comprehensive_search:
    description: "Extended search space for thorough optimization"
    models:
      random_forest:
        n_estimators: {type: "int", low: 50, high: 1000, step: 50}
        max_depth: {type: "int", low: 3, high: 30}
        min_samples_split: {type: "int", low: 2, high: 50}
        min_samples_leaf: {type: "int", low: 1, high: 20}
      
      xgboost:
        n_estimators: {type: "int", low: 50, high: 1000, step: 50}
        max_depth: {type: "int", low: 3, high: 15}
        learning_rate: {type: "float", low: 0.001, high: 0.5, log: true}
        subsample: {type: "float", low: 0.5, high: 1.0}
        colsample_bytree: {type: "float", low: 0.5, high: 1.0}
